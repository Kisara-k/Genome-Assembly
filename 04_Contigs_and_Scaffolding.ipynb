{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc539a9",
   "metadata": {},
   "source": [
    "# Genome Assembly: 4. Contigs and Scaffolding\n",
    "\n",
    "## Overview\n",
    "This notebook covers the post-graph steps to turn De Bruijn graphs into assembled genomes:\n",
    "\n",
    "1. **Contigs**: Linear sequences from unambiguous graph regions\n",
    "2. **Paired-end reads**: Using read pairs to resolve ambiguities and span repeats\n",
    "3. **Scaffolding**: Linking contigs using pair distance information\n",
    "4. **Gap filling**: Closing gaps between scaffolded contigs\n",
    "\n",
    "**Key insight**: Paired-end reads tell us that two sequences are nearby in the genome, even if we can't directly sequence the region between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256b869",
   "metadata": {},
   "source": [
    "## 1. Contig Extraction from De Bruijn Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b84ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "from typing import List, Tuple, Set, Dict, Optional\n",
    "import math\n",
    "\n",
    "class ContigExtractor:\n",
    "    \"\"\"\n",
    "    Extract contigs from a De Bruijn graph.\n",
    "    A contig is a linear path where all internal nodes have in-degree = out-degree = 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k: int):\n",
    "        self.k = k\n",
    "        self.graph = defaultdict(list)  # node -> [(next_node, edge_label, count)]\n",
    "        self.reverse_graph = defaultdict(list)  # for finding in-edges\n",
    "        self.in_degree = defaultdict(int)\n",
    "        self.out_degree = defaultdict(int)\n",
    "        self.coverage = defaultdict(float)\n",
    "    \n",
    "    def add_edge(self, prefix: str, suffix: str, base: str, count: int = 1):\n",
    "        \"\"\"\n",
    "        Add an edge to the graph (represents a k-mer).\n",
    "        \"\"\"\n",
    "        self.graph[prefix].append((suffix, base, count))\n",
    "        self.reverse_graph[suffix].append((prefix, base, count))\n",
    "        self.out_degree[prefix] += count\n",
    "        self.in_degree[suffix] += count\n",
    "    \n",
    "    def extract_contigs(self) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract all contigs from the graph.\n",
    "        Returns list of (sequence, avg_coverage) tuples.\n",
    "        \"\"\"\n",
    "        contigs = []\n",
    "        visited_edges = set()\n",
    "        \n",
    "        # Find all possible starting points\n",
    "        all_nodes = set(self.graph.keys()) | set(self.reverse_graph.keys())\n",
    "        \n",
    "        for start_node in all_nodes:\n",
    "            # Start from nodes with in-degree = 0 or in-degree != out-degree\n",
    "            in_deg = self.in_degree[start_node]\n",
    "            out_deg = self.out_degree[start_node]\n",
    "            \n",
    "            if out_deg == 0:\n",
    "                continue  # Dead end, can't start here\n",
    "            \n",
    "            # Try each outgoing edge from this node\n",
    "            for next_node, base, count in self.graph[start_node]:\n",
    "                edge_key = (start_node, next_node, base)\n",
    "                \n",
    "                if edge_key in visited_edges:\n",
    "                    continue\n",
    "                \n",
    "                # Trace the contig from here\n",
    "                contig, coverage = self._trace_contig(start_node, next_node, base, count, visited_edges)\n",
    "                \n",
    "                if contig:\n",
    "                    contigs.append((contig, coverage))\n",
    "        \n",
    "        return contigs\n",
    "    \n",
    "    def _trace_contig(self, start: str, next_node: str, first_base: str, first_count: int,\n",
    "                     visited: Set) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Trace a linear path (contig) through the graph.\n",
    "        Stops when reaching a branching point or cycle.\n",
    "        \"\"\"\n",
    "        sequence = start + first_base\n",
    "        current = next_node\n",
    "        counts = [first_count]\n",
    "        visited.add((start, next_node, first_base))\n",
    "        \n",
    "        # Continue while path is linear (in-degree = out-degree = 1)\n",
    "        while True:\n",
    "            in_deg = self.in_degree[current]\n",
    "            out_deg = self.out_degree[current]\n",
    "            \n",
    "            if in_deg == 0 or out_deg == 0 or out_deg > 1:\n",
    "                break  # Branching or dead end\n",
    "            \n",
    "            # Get the next edge\n",
    "            if not self.graph[current]:\n",
    "                break\n",
    "            \n",
    "            next_next, next_base, count = self.graph[current][0]\n",
    "            edge_key = (current, next_next, next_base)\n",
    "            \n",
    "            if edge_key in visited:\n",
    "                break  # Would create a cycle\n",
    "            \n",
    "            sequence += next_base\n",
    "            counts.append(count)\n",
    "            visited.add(edge_key)\n",
    "            current = next_next\n",
    "        \n",
    "        # Calculate average coverage from edge counts\n",
    "        avg_coverage = np.mean(counts) if counts else 0\n",
    "        \n",
    "        return sequence, avg_coverage\n",
    "    \n",
    "    def filter_contigs(self, contigs: List[Tuple[str, float]], \n",
    "                      min_length: int = 100, min_coverage: float = 2.0) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Filter contigs by length and coverage.\n",
    "        Short, low-coverage contigs are likely errors.\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        for seq, cov in contigs:\n",
    "            if len(seq) >= min_length and cov >= min_coverage:\n",
    "                filtered.append((seq, cov))\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "# Test contig extraction\n",
    "print(\"Contig Extraction from De Bruijn Graph\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a simple test graph\n",
    "extractor = ContigExtractor(k=21)\n",
    "\n",
    "# Add edges representing a simple genome\n",
    "test_sequence = \"ATGCGATCGATCGATCGATCG\"\n",
    "for i in range(len(test_sequence) - 20):\n",
    "    prefix = test_sequence[i:i+20]\n",
    "    suffix = test_sequence[i+1:i+21]\n",
    "    base = test_sequence[i+20]\n",
    "    count = 5  # 5x coverage\n",
    "    extractor.add_edge(prefix, suffix, base, count)\n",
    "\n",
    "contigs = extractor.extract_contigs()\n",
    "contigs = extractor.filter_contigs(contigs, min_length=10, min_coverage=1)\n",
    "\n",
    "print(f\"Test sequence: {test_sequence}\")\n",
    "print(f\"\\nExtracted {len(contigs)} contig(s):\")\n",
    "for i, (seq, cov) in enumerate(contigs):\n",
    "    print(f\"  Contig {i+1}: {seq}\")\n",
    "    print(f\"    Length: {len(seq)} bp, Avg coverage: {cov:.1f}x\")\n",
    "    if seq in test_sequence:\n",
    "        print(f\"    ✓ Matches original sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f328716",
   "metadata": {},
   "source": [
    "## 2. Paired-End Reads and Insert Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedEndRead:\n",
    "    \"\"\"\n",
    "    Represents a pair of reads from a DNA fragment.\n",
    "    Key: We know the distance between read 1 and read 2 (insert size).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, read1_id: str, read1_seq: str, read2_id: str, read2_seq: str,\n",
    "                 insert_size_mean: int = 500, insert_size_std: int = 50):\n",
    "        self.read1_id = read1_id\n",
    "        self.read1_seq = read1_seq\n",
    "        self.read2_id = read2_id\n",
    "        self.read2_seq = read2_seq\n",
    "        self.insert_size_mean = insert_size_mean\n",
    "        self.insert_size_std = insert_size_std\n",
    "    \n",
    "    def get_span(self, read1_len: int, read2_len: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Get the expected minimum and maximum genomic distance between read 1 end and read 2 start.\n",
    "        span_min = insert_size - std - read_lengths\n",
    "        span_max = insert_size + std - read_lengths\n",
    "        \"\"\"\n",
    "        min_span = max(0, self.insert_size_mean - 3*self.insert_size_std - read1_len - read2_len)\n",
    "        max_span = self.insert_size_mean + 3*self.insert_size_std - read1_len - read2_len\n",
    "        return min_span, max_span\n",
    "\n",
    "def generate_paired_reads(sequence: str, read_length: int = 100, insert_size_mean: int = 500,\n",
    "                         insert_size_std: int = 50, coverage: int = 10, error_rate: float = 0.01,\n",
    "                         seed: int = 42) -> List[PairedEndRead]:\n",
    "    \"\"\"\n",
    "    Generate paired-end reads from a sequence.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    num_pairs = (len(sequence) * coverage) // (2 * read_length)\n",
    "    pairs = []\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    \n",
    "    for i in range(num_pairs):\n",
    "        # Random fragment position\n",
    "        fragment_start = random.randint(0, len(sequence) - insert_size_mean)\n",
    "        \n",
    "        # Read 1 (forward)\n",
    "        read1_seq = sequence[fragment_start:fragment_start + read_length]\n",
    "        \n",
    "        # Read 2 (reverse complement from other end)\n",
    "        read2_start = fragment_start + insert_size_mean - read_length\n",
    "        read2_seq = sequence[read2_start:read2_start + read_length]\n",
    "        \n",
    "        # Introduce errors\n",
    "        def add_errors(seq, rate):\n",
    "            seq_list = list(seq)\n",
    "            for j in range(len(seq_list)):\n",
    "                if random.random() < rate:\n",
    "                    seq_list[j] = random.choice([b for b in bases if b != seq_list[j]])\n",
    "            return ''.join(seq_list)\n",
    "        \n",
    "        read1_seq = add_errors(read1_seq, error_rate)\n",
    "        read2_seq = add_errors(read2_seq, error_rate)\n",
    "        \n",
    "        pair = PairedEndRead(\n",
    "            f\"read{i}/1\", read1_seq,\n",
    "            f\"read{i}/2\", read2_seq,\n",
    "            insert_size_mean, insert_size_std\n",
    "        )\n",
    "        pairs.append(pair)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Test paired-end generation\n",
    "print(\"\\nPaired-End Reads\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_seq = \"ATGCGATCGATCGATCGATCG\" * 30  # 660 bp\n",
    "test_pairs = generate_paired_reads(test_seq, read_length=100, insert_size_mean=500,\n",
    "                                insert_size_std=50, coverage=5)\n",
    "\n",
    "print(f\"Generated {len(test_pairs)} paired-end read pairs\")\n",
    "print(f\"\\nExample pair:\")\n",
    "pair = test_pairs[0]\n",
    "print(f\"  Read 1: {pair.read1_seq[:50]}... ({len(pair.read1_seq)}bp)\")\n",
    "print(f\"  Read 2: {pair.read2_seq[:50]}... ({len(pair.read2_seq)}bp)\")\n",
    "print(f\"  Insert size: {pair.insert_size_mean}bp ± {pair.insert_size_std}bp\")\n",
    "min_span, max_span = pair.get_span(len(pair.read1_seq), len(pair.read2_seq))\n",
    "print(f\"  Expected gap between reads: {min_span}bp to {max_span}bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a62be4",
   "metadata": {},
   "source": [
    "## 3. Scaffolding: Linking Contigs with Pair Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldLink:\n",
    "    \"\"\"\n",
    "    Represents a connection between two contigs based on paired reads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, contig1_id: int, contig2_id: int, \n",
    "                 num_pairs: int, gap_estimate: float, gap_std: float,\n",
    "                 orientation: str = '+-'):\n",
    "        self.contig1_id = contig1_id\n",
    "        self.contig2_id = contig2_id\n",
    "        self.num_pairs = num_pairs  # Supporting pairs\n",
    "        self.gap_estimate = gap_estimate  # Estimated gap size\n",
    "        self.gap_std = gap_std\n",
    "        self.orientation = orientation  # '++', '+-', '-+', '--'\n",
    "    \n",
    "    def is_confident(self, min_support: int = 5) -> bool:\n",
    "        \"\"\"Link is confident if supported by enough read pairs.\"\"\"\n",
    "        return self.num_pairs >= min_support\n",
    "\n",
    "class Scaffolder:\n",
    "    \"\"\"\n",
    "    Build scaffolds from contigs using paired-end information.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Map reads to contigs\n",
    "    2. Find pairs where read1 and read2 map to different contigs\n",
    "    3. Calculate gap size from insert size and contig positions\n",
    "    4. Build graph of contigs connected by pairs\n",
    "    5. Find linear paths (scaffolds)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, contigs: List[str]):\n",
    "        self.contigs = contigs  # List of contig sequences\n",
    "        self.contig_positions = self._build_contig_map()\n",
    "        self.scaffold_links = []\n",
    "    \n",
    "    def _build_contig_map(self) -> Dict[int, Dict]:\n",
    "        \"\"\"\n",
    "        Build a map of contig IDs to their sequences and lengths.\n",
    "        \"\"\"\n",
    "        positions = {}\n",
    "        for i, contig in enumerate(self.contigs):\n",
    "            positions[i] = {'seq': contig, 'length': len(contig)}\n",
    "        return positions\n",
    "    \n",
    "    def find_read_positions(self, read_seq: str, contig_id: int, \n",
    "                           min_match: int = 20) -> Optional[Tuple[int, int, bool]]:\n",
    "        \"\"\"\n",
    "        Find where a read maps to a contig.\n",
    "        Returns (start_position, end_position, is_reverse_complement).\n",
    "        Simple exact/near-exact matching.\n",
    "        \"\"\"\n",
    "        contig = self.contig_positions[contig_id]['seq']\n",
    "        \n",
    "        # Forward strand\n",
    "        for i in range(len(contig) - min_match + 1):\n",
    "            if contig[i:].startswith(read_seq[:min_match]):\n",
    "                return (i, i + len(read_seq), False)\n",
    "        \n",
    "        # Reverse complement (simplified: just check reverse)\n",
    "        read_rev = read_seq[::-1]\n",
    "        for i in range(len(contig) - min_match + 1):\n",
    "            if contig[i:].startswith(read_rev[:min_match]):\n",
    "                return (i, i + len(read_seq), True)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def build_scaffold_links(self, pairs: List[PairedEndRead], min_support: int = 5):\n",
    "        \"\"\"\n",
    "        Build scaffold links by finding pairs that map to different contigs.\n",
    "        \"\"\"\n",
    "        pair_links = defaultdict(list)  # (contig1, contig2) -> list of (gap_size, support_count)\n",
    "        \n",
    "        for pair in pairs:\n",
    "            positions1 = None\n",
    "            positions2 = None\n",
    "            contig1_id = None\n",
    "            contig2_id = None\n",
    "            \n",
    "            # Find mapping for read 1\n",
    "            for cid in range(len(self.contigs)):\n",
    "                pos = self.find_read_positions(pair.read1_seq, cid, min_match=15)\n",
    "                if pos:\n",
    "                    positions1 = pos\n",
    "                    contig1_id = cid\n",
    "                    break\n",
    "            \n",
    "            # Find mapping for read 2\n",
    "            for cid in range(len(self.contigs)):\n",
    "                pos = self.find_read_positions(pair.read2_seq, cid, min_match=15)\n",
    "                if pos:\n",
    "                    positions2 = pos\n",
    "                    contig2_id = cid\n",
    "                    break\n",
    "            \n",
    "            # If on different contigs, record the link\n",
    "            if (positions1 and positions2 and contig1_id is not None and \n",
    "                contig2_id is not None and contig1_id != contig2_id):\n",
    "                \n",
    "                # Calculate expected gap\n",
    "                read1_end = positions1[1]\n",
    "                read2_start = positions2[0]\n",
    "                gap = pair.insert_size_mean - len(pair.read1_seq) - len(pair.read2_seq)\n",
    "                \n",
    "                key = tuple(sorted([contig1_id, contig2_id]))\n",
    "                pair_links[key].append(gap)\n",
    "        \n",
    "        # Aggregate links\n",
    "        for (c1, c2), gaps in pair_links.items():\n",
    "            if len(gaps) >= min_support:\n",
    "                gap_mean = np.mean(gaps)\n",
    "                gap_std = np.std(gaps)\n",
    "                link = ScaffoldLink(c1, c2, len(gaps), gap_mean, gap_std)\n",
    "                self.scaffold_links.append(link)\n",
    "    \n",
    "    def build_scaffolds(self) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Build scaffolds (linear sequences of contigs) from links.\n",
    "        \"\"\"\n",
    "        # Build adjacency graph\n",
    "        adj = defaultdict(list)\n",
    "        for link in self.scaffold_links:\n",
    "            adj[link.contig1_id].append(link.contig2_id)\n",
    "            adj[link.contig2_id].append(link.contig1_id)\n",
    "        \n",
    "        # Find linear paths\n",
    "        scaffolds = []\n",
    "        visited = set()\n",
    "        \n",
    "        for start in range(len(self.contigs)):\n",
    "            if start in visited or len(adj[start]) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find the path starting from this node\n",
    "            # (simplified: just follow single edges)\n",
    "            path = [start]\n",
    "            visited.add(start)\n",
    "            current = start\n",
    "            \n",
    "            while len(adj[current]) > 0:\n",
    "                next_node = None\n",
    "                for neighbor in adj[current]:\n",
    "                    if neighbor not in visited:\n",
    "                        next_node = neighbor\n",
    "                        break\n",
    "                \n",
    "                if next_node is None:\n",
    "                    break\n",
    "                \n",
    "                path.append(next_node)\n",
    "                visited.add(next_node)\n",
    "                current = next_node\n",
    "            \n",
    "            scaffolds.append(path)\n",
    "        \n",
    "        return scaffolds\n",
    "\n",
    "# Test scaffolding\n",
    "print(\"\\nScaffolding Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test contigs (broken by repeats)\n",
    "contig1 = \"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "contig2 = \"GGCGTAGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGA\"\n",
    "contig3 = \"TTACGTACGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGAT\"\n",
    "contigs = [contig1, contig2, contig3]\n",
    "\n",
    "print(f\"Test contigs:\")\n",
    "for i, c in enumerate(contigs):\n",
    "    print(f\"  Contig {i}: {len(c)}bp\")\n",
    "\n",
    "# Scaffold them\n",
    "scaffolder = Scaffolder(contigs)\n",
    "scaffolder.build_scaffold_links(test_pairs, min_support=2)\n",
    "\n",
    "print(f\"\\nFound {len(scaffolder.scaffold_links)} scaffold links\")\n",
    "for link in scaffolder.scaffold_links:\n",
    "    print(f\"  Contig {link.contig1_id} <-> Contig {link.contig2_id}: \"\n",
    "          f\"{link.num_pairs} pairs, gap={link.gap_estimate:.0f}±{link.gap_std:.0f}bp\")\n",
    "\n",
    "scaffolds = scaffolder.build_scaffolds()\n",
    "print(f\"\\nBuilt {len(scaffolds)} scaffold(s):\")\n",
    "for i, scaffold in enumerate(scaffolds):\n",
    "    print(f\"  Scaffold {i}: Contigs {scaffold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402075c",
   "metadata": {},
   "source": [
    "## 4. Gap Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapFiller:\n",
    "    \"\"\"\n",
    "    Fill gaps between scaffolded contigs.\n",
    "    \n",
    "    Strategies:\n",
    "    1. Assemble reads that map across the gap\n",
    "    2. Use path-finding in the De Bruijn graph\n",
    "    3. Interpolate with N's if assembly fails\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reads: List[str], k: int = 21):\n",
    "        self.reads = reads\n",
    "        self.k = k\n",
    "        self.kmers = self._build_kmer_map()\n",
    "    \n",
    "    def _build_kmer_map(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Build a map of k-mers to their frequencies.\n",
    "        \"\"\"\n",
    "        kmer_map = defaultdict(int)\n",
    "        for read in self.reads:\n",
    "            for i in range(len(read) - self.k + 1):\n",
    "                kmer = read[i:i+self.k]\n",
    "                kmer_map[kmer] += 1\n",
    "        return kmer_map\n",
    "    \n",
    "    def find_path(self, start_kmer: str, end_kmer: str, max_length: int = 1000,\n",
    "                 min_coverage: int = 2) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Find a path in the De Bruijn graph from start_kmer to end_kmer.\n",
    "        Uses BFS with coverage constraints.\n",
    "        \"\"\"\n",
    "        if start_kmer not in self.kmers or end_kmer not in self.kmers:\n",
    "            return None\n",
    "        \n",
    "        if self.kmers[start_kmer] < min_coverage or self.kmers[end_kmer] < min_coverage:\n",
    "            return None  # Low coverage k-mers, likely errors\n",
    "        \n",
    "        # BFS from start_kmer\n",
    "        queue = deque([(start_kmer, start_kmer)])\n",
    "        visited = {start_kmer}\n",
    "        \n",
    "        while queue:\n",
    "            current_path, current_kmer = queue.popleft()\n",
    "            \n",
    "            if len(current_path) > max_length + self.k:\n",
    "                continue  # Path getting too long\n",
    "            \n",
    "            # Generate next k-mers by extending one base\n",
    "            for base in ['A', 'T', 'G', 'C']:\n",
    "                next_kmer = current_kmer[1:] + base\n",
    "                \n",
    "                if next_kmer in visited:\n",
    "                    continue\n",
    "                if self.kmers[next_kmer] < min_coverage:\n",
    "                    continue\n",
    "                \n",
    "                new_path = current_path + base\n",
    "                \n",
    "                # Check if we reached the end\n",
    "                if next_kmer == end_kmer:\n",
    "                    return new_path\n",
    "                \n",
    "                visited.add(next_kmer)\n",
    "                queue.append((new_path, next_kmer))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fill_gap(self, contig1: str, contig2: str, gap_size_estimate: int,\n",
    "                gap_size_std: float) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Attempt to fill a gap between two contigs.\n",
    "        \"\"\"\n",
    "        # Get end of contig1 and start of contig2 as anchors\n",
    "        end_anchor = contig1[-self.k:]\n",
    "        start_anchor = contig2[:self.k]\n",
    "        \n",
    "        # Check if they're directly connected\n",
    "        if end_anchor[1:] == start_anchor[:-1]:\n",
    "            # Contigs are directly adjacent\n",
    "            return \"\"\n",
    "        \n",
    "        # Search for a path\n",
    "        max_search = int(gap_size_estimate + 3*gap_size_std + 2*self.k)\n",
    "        gap_sequence = self.find_path(end_anchor, start_anchor, max_length=max_search)\n",
    "        \n",
    "        return gap_sequence\n",
    "\n",
    "# Test gap filling\n",
    "print(\"\\nGap Filling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create contigs with a gap\n",
    "seq1 = \"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "seq_gap = \"TTTTAAAATTTTAAAA\"  # True gap sequence\n",
    "seq2 = \"GATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "\n",
    "full_seq = seq1 + seq_gap + seq2\n",
    "\n",
    "# Generate reads from the full sequence\n",
    "gap_reads = generate_paired_reads(full_seq, read_length=50, coverage=10)[0]\n",
    "all_reads_gap = [gap_reads.read1_seq for gap_reads in generate_paired_reads(full_seq, read_length=50, coverage=10)]\n",
    "all_reads_gap += [gap_reads.read2_seq for gap_reads in generate_paired_reads(full_seq, read_length=50, coverage=10)]\n",
    "\n",
    "print(f\"Contig 1 (end): ...{seq1[-30:]}\")\n",
    "print(f\"True gap:       {seq_gap}\")\n",
    "print(f\"Contig 2 (start): {seq2[:30]}...\")\n",
    "print(f\"\\nTotal gap size: {len(seq_gap)} bp\")\n",
    "\n",
    "filler = GapFiller(all_reads_gap, k=15)\n",
    "filled_gap = filler.fill_gap(seq1, seq2, len(seq_gap), len(seq_gap) * 0.1)\n",
    "\n",
    "if filled_gap:\n",
    "    print(f\"\\nFilled gap: {filled_gap}\")\n",
    "    if filled_gap == seq_gap:\n",
    "        print(\"✓ Perfect match!\")\n",
    "    else:\n",
    "        print(f\"✗ Gap mismatch (got {len(filled_gap)}bp instead of {len(seq_gap)}bp)\")\n",
    "else:\n",
    "    print(\"Gap filling failed - would use N's as placeholder\")\n",
    "    print(f\"Placeholder: {seq1}{'N'*len(seq_gap)}{seq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef9ad8",
   "metadata": {},
   "source": [
    "## 5. Complete Assembly Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93987906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta(contigs: List[Tuple[str, str]], filename: str):\n",
    "    \"\"\"\n",
    "    Write contigs to FASTA file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for header, seq in contigs:\n",
    "            f.write(f\">{header}\\n\")\n",
    "            # Write sequence in 80bp lines\n",
    "            for i in range(0, len(seq), 80):\n",
    "                f.write(seq[i:i+80] + '\\n')\n",
    "\n",
    "def assemble_complete(reads: List[str], pairs: List[PairedEndRead], \n",
    "                     k: int = 21, output_prefix: str = \"assembly\") -> dict:\n",
    "    \"\"\"\n",
    "    Complete assembly pipeline.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    print(f\"Complete Assembly Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nInput:\")\n",
    "    print(f\"  Reads: {len(reads)}\")\n",
    "    print(f\"  Pairs: {len(pairs)}\")\n",
    "    \n",
    "    # Step 1: Build De Bruijn graph and extract contigs\n",
    "    print(f\"\\nStep 1: Extract contigs from De Bruijn graph...\")\n",
    "    extractor = ContigExtractor(k=k)\n",
    "    \n",
    "    from collections import Counter\n",
    "    for read in reads:\n",
    "        for i in range(len(read) - k + 1):\n",
    "            kmer = read[i:i+k]\n",
    "            prefix = kmer[:-1]\n",
    "            suffix = kmer[1:]\n",
    "            base = kmer[-1]\n",
    "            extractor.add_edge(prefix, suffix, base, count=1)\n",
    "    \n",
    "    raw_contigs = extractor.extract_contigs()\n",
    "    contigs = extractor.filter_contigs(raw_contigs, min_length=100, min_coverage=1.0)\n",
    "    print(f\"  Generated {len(raw_contigs)} raw contigs\")\n",
    "    print(f\"  Filtered to {len(contigs)} high-quality contigs\")\n",
    "    stats['raw_contigs'] = len(raw_contigs)\n",
    "    stats['filtered_contigs'] = len(contigs)\n",
    "    \n",
    "    # Step 2: Scaffold with paired reads\n",
    "    print(f\"\\nStep 2: Build scaffolds with paired-end reads...\")\n",
    "    contig_seqs = [seq for seq, _ in contigs]\n",
    "    scaffolder = Scaffolder(contig_seqs)\n",
    "    scaffolder.build_scaffold_links(pairs, min_support=3)\n",
    "    scaffolds = scaffolder.build_scaffolds()\n",
    "    print(f\"  Found {len(scaffolder.scaffold_links)} scaffold links\")\n",
    "    print(f\"  Built {len(scaffolds)} scaffolds\")\n",
    "    stats['scaffolds'] = len(scaffolds)\n",
    "    \n",
    "    # Step 3: Gap filling\n",
    "    print(f\"\\nStep 3: Fill gaps...\")\n",
    "    final_sequences = []\n",
    "    for scaffold in scaffolds:\n",
    "        if len(scaffold) == 1:\n",
    "            final_sequences.append(contig_seqs[scaffold[0]])\n",
    "        else:\n",
    "            # Join contigs with N's (simplified, no actual gap filling)\n",
    "            parts = [contig_seqs[cid] for cid in scaffold]\n",
    "            joined = 'NNNNNNNNNN'.join(parts)\n",
    "            final_sequences.append(joined)\n",
    "    \n",
    "    print(f\"  Generated {len(final_sequences)} final sequences\")\n",
    "    stats['final_sequences'] = len(final_sequences)\n",
    "    \n",
    "    # Write output\n",
    "    fasta_output = [(f\"contig_{i}\", seq) for i, seq in enumerate(final_sequences)]\n",
    "    fasta_file = f\"{output_prefix}.fasta\"\n",
    "    write_fasta(fasta_output, fasta_file)\n",
    "    print(f\"\\nOutput written to {fasta_file}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_assembled = sum(len(seq) for seq in final_sequences)\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total length: {total_assembled} bp\")\n",
    "    print(f\"  Number of sequences: {len(final_sequences)}\")\n",
    "    print(f\"  N50 (rough): {np.median([len(s) for s in final_sequences])} bp\")\n",
    "    stats['total_length'] = total_assembled\n",
    "    stats['n50_estimate'] = np.median([len(s) for s in final_sequences])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run complete assembly\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "assembly_reads = [r.read1_seq for r in test_pairs] + [r.read2_seq for r in test_pairs]\n",
    "assembly_stats = assemble_complete(assembly_reads, test_pairs, k=15, output_prefix=\"test_assembly\")\n",
    "print(f\"\\nAssembly complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5dc06",
   "metadata": {},
   "source": [
    "## 6. Output Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCommon Assembly Output Formats\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. FASTA Format (.fasta or .fa)\n",
    "   - Simple text format with sequences\n",
    "   - Header line starts with >\n",
    "   - Sequences in 80bp lines (convention)\n",
    "   \n",
    "   Example:\n",
    "   > contig_1 len=5000\n",
    "   ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATC\n",
    "   ATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\n",
    "   ...\n",
    "\n",
    "2. FASTA Quality (.fastq or .qual)\n",
    "   - FASTA format with quality scores\n",
    "   - Header, sequence, +, quality scores\n",
    "\n",
    "3. Scaffold File (.scf or .agp)\n",
    "   - Documents how contigs are linked\n",
    "   - Shows gaps and overlaps\n",
    "   - Format: scaffold_id contig_start contig_end contig_id contig_start contig_end orientation\n",
    "\n",
    "4. GFF/GTF (with assembly info)\n",
    "   - Can annotate contigs with features\n",
    "   - Tracks which reads/contigs contributed\n",
    "\n",
    "5. Sam/BAM\n",
    "   - Maps original reads back to assembly\n",
    "   - Useful for coverage analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nReading assembly files:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def read_fasta(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Read a FASTA file and return list of (header, sequence) tuples.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    current_header = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence\n",
    "                if current_header is not None:\n",
    "                    sequences.append((current_header, ''.join(current_seq)))\n",
    "                \n",
    "                current_header = line[1:]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Save last sequence\n",
    "        if current_header is not None:\n",
    "            sequences.append((current_header, ''.join(current_seq)))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "try:\n",
    "    read_seqs = read_fasta(\"test_assembly.fasta\")\n",
    "    print(f\"\\nRead {len(read_seqs)} sequences from test_assembly.fasta\")\n",
    "    for header, seq in read_seqs:\n",
    "        print(f\"  {header}: {len(seq)} bp\")\n",
    "except FileNotFoundError:\n",
    "    print(\"test_assembly.fasta not found (expected if assembly step was skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dfd63",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**This notebook covered:**\n",
    "- ✓ Contig extraction from De Bruijn graphs\n",
    "- ✓ Paired-end reads and insert size estimation\n",
    "- ✓ Scaffolding: using pairs to link contigs\n",
    "- ✓ Gap filling: reconstructing sequences between contigs\n",
    "- ✓ Output formats (FASTA, etc.)\n",
    "\n",
    "**Key insights:**\n",
    "- Paired-end reads resolve ambiguities that single reads can't\n",
    "- Scaffolding creates longer sequences from shorter contigs\n",
    "- Gap filling is challenging and often results in placeholders (N's)\n",
    "- Modern assemblers often produce scaffolds, not complete genomes\n",
    "\n",
    "**Next notebook (5) will cover:**\n",
    "- Assembly quality metrics (N50, L50, etc.)\n",
    "- Comparing assemblies\n",
    "- Common issues and troubleshooting\n",
    "- When to use different assemblers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
