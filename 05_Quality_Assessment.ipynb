{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bcadf1",
   "metadata": {},
   "source": [
    "# Genome Assembly: 5. Assembly Quality Assessment\n",
    "\n",
    "## Overview\n",
    "Evaluating assembly quality is crucial. This notebook covers:\n",
    "\n",
    "1. **Contiguity metrics**: N50, L50, NG50, etc.\n",
    "2. **Accuracy metrics**: Comparing to reference (when available)\n",
    "3. **Completeness**: What fraction of the genome is assembled?\n",
    "4. **Coverage analysis**: Is coverage uniform?\n",
    "5. **Repeat resolution**: How well were repeats handled?\n",
    "6. **Comparing assemblies**: Which is better?\n",
    "\n",
    "**Philosophy**: No single metric tells the whole storyâ€”use multiple metrics together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162fcfa",
   "metadata": {},
   "source": [
    "## 1. Contiguity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def calculate_contiguity_metrics(contig_lengths: List[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate standard contiguity metrics.\n",
    "    \n",
    "    Key metrics:\n",
    "    - N50: Length of the contig for which all contigs >= this length comprise 50% of the genome\n",
    "    - N75, N90: Same but for 75% and 90%\n",
    "    - L50: Number of contigs needed to achieve N50\n",
    "    - NGA50: Like N50 but measured against assembly length (genome-agnostic)\n",
    "    \"\"\"\n",
    "    if not contig_lengths:\n",
    "        return {}\n",
    "    \n",
    "    sorted_lengths = sorted(contig_lengths, reverse=True)\n",
    "    total_length = sum(sorted_lengths)\n",
    "    \n",
    "    def get_nx_lx(x: float) -> Tuple[int, int]:\n",
    "        \"\"\"Get N{x} and L{x}\"\"\"\n",
    "        cumsum = 0\n",
    "        for i, length in enumerate(sorted_lengths):\n",
    "            cumsum += length\n",
    "            if cumsum >= (total_length * x / 100):\n",
    "                return length, i + 1\n",
    "        return 0, len(sorted_lengths)\n",
    "    \n",
    "    n50, l50 = get_nx_lx(50)\n",
    "    n75, l75 = get_nx_lx(75)\n",
    "    n90, l90 = get_nx_lx(90)\n",
    "    \n",
    "    return {\n",
    "        'num_contigs': len(sorted_lengths),\n",
    "        'total_length': total_length,\n",
    "        'mean_length': np.mean(sorted_lengths),\n",
    "        'median_length': np.median(sorted_lengths),\n",
    "        'min_length': min(sorted_lengths),\n",
    "        'max_length': max(sorted_lengths),\n",
    "        'std_dev': np.std(sorted_lengths),\n",
    "        'n50': n50,\n",
    "        'l50': l50,\n",
    "        'n75': n75,\n",
    "        'l75': l75,\n",
    "        'n90': n90,\n",
    "        'l90': l90,\n",
    "        'sorted_lengths': sorted_lengths\n",
    "    }\n",
    "\n",
    "def print_contiguity_report(metrics: dict):\n",
    "    \"\"\"\n",
    "    Print a formatted contiguity report.\n",
    "    \"\"\"\n",
    "    print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘              ASSEMBLY CONTIGUITY METRICS                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Basic Statistics:\n",
    "  Number of contigs:        {metrics['num_contigs']:>8}\n",
    "  Total length:             {metrics['total_length']:>8,} bp\n",
    "  Mean length:              {metrics['mean_length']:>8.1f} bp\n",
    "  Median length:            {metrics['median_length']:>8.0f} bp\n",
    "  Std. deviation:           {metrics['std_dev']:>8.1f} bp\n",
    "  Min/Max length:           {metrics['min_length']:>8}/{metrics['max_length']:<8} bp\n",
    "\n",
    "N-Statistics (Nx = contig length for which contigs >= this length comprise x% of assembly):\n",
    "  N50 (50%):                {metrics['n50']:>8} bp  (L50={metrics['l50']})\n",
    "  N75 (75%):                {metrics['n75']:>8} bp  (L75={metrics['l75']})\n",
    "  N90 (90%):                {metrics['n90']:>8} bp  (L90={metrics['l90']})\n",
    "\n",
    "Interpretation:\n",
    "  â€¢ N50 is the most commonly used metric for assembly quality\n",
    "  â€¢ Higher N50 is generally better\n",
    "  â€¢ N50 > 50,000 bp is good for bacterial genomes\n",
    "  â€¢ N50 > 1,000,000 bp is good for mammalian genomes\n",
    "  â€¢ L50 tells you how many large contigs you have\n",
    "\"\"\")\n",
    "\n",
    "# Test with example assemblies\n",
    "test_assembly1 = [100000, 95000, 80000, 75000, 70000, 60000, 50000, 40000, 30000, 20000]\n",
    "test_assembly2 = [150000, 140000, 130000, 5000, 4000, 3000, 2000, 1000, 500, 100]\n",
    "test_assembly3 = [20000] * 100  # Fragmented assembly\n",
    "\n",
    "metrics1 = calculate_contiguity_metrics(test_assembly1)\n",
    "metrics2 = calculate_contiguity_metrics(test_assembly2)\n",
    "metrics3 = calculate_contiguity_metrics(test_assembly3)\n",
    "\n",
    "print(\"ASSEMBLY 1: Balanced contigs\")\n",
    "print_contiguity_report(metrics1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSEMBLY 2: Few large + many small (typical)\")\n",
    "print_contiguity_report(metrics2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSEMBLY 3: Highly fragmented\")\n",
    "print_contiguity_report(metrics3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(f\"Assembly 1 N50: {metrics1['n50']} (good balance)\")\n",
    "print(f\"Assembly 2 N50: {metrics2['n50']} (very good)\")\n",
    "print(f\"Assembly 3 N50: {metrics3['n50']} (fragmented)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a1bb1",
   "metadata": {},
   "source": [
    "## 2. Visualization of Contig Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contig_distribution(metrics: dict, title: str = \"Contig Size Distribution\"):\n",
    "    \"\"\"\n",
    "    Plot contig size distribution and cumulative plot.\n",
    "    \"\"\"\n",
    "    lengths = metrics['sorted_lengths']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Histogram of contig sizes\n",
    "    ax = axes[0]\n",
    "    ax.hist(lengths, bins=30, color='steelblue', edgecolor='black', log=True)\n",
    "    ax.axvline(metrics['n50'], color='red', linestyle='--', linewidth=2, label=f\"N50={metrics['n50']}\")\n",
    "    ax.axvline(metrics['median_length'], color='green', linestyle='--', linewidth=2, label=f\"Median={metrics['median_length']:.0f}\")\n",
    "    ax.set_xlabel('Contig Length (bp)')\n",
    "    ax.set_ylabel('Number of Contigs (log scale)')\n",
    "    ax.set_title('Contig Length Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Cumulative length plot\n",
    "    ax = axes[1]\n",
    "    cumsum = np.cumsum(lengths)\n",
    "    x = np.arange(1, len(lengths) + 1)\n",
    "    ax.plot(x, cumsum, color='steelblue', linewidth=2, marker='o', markersize=3)\n",
    "    ax.axhline(sum(lengths) * 0.5, color='red', linestyle='--', alpha=0.5, label='50%')\n",
    "    ax.axhline(sum(lengths) * 0.75, color='orange', linestyle='--', alpha=0.5, label='75%')\n",
    "    ax.axhline(sum(lengths) * 0.9, color='purple', linestyle='--', alpha=0.5, label='90%')\n",
    "    ax.set_xlabel('Number of Contigs')\n",
    "    ax.set_ylabel('Cumulative Length (bp)')\n",
    "    ax.set_title('Cumulative Contig Length')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Log-scale cumulative plot\n",
    "    ax = axes[2]\n",
    "    ax.loglog(x, cumsum, color='steelblue', linewidth=2, marker='o', markersize=3)\n",
    "    ax.axhline(sum(lengths) * 0.5, color='red', linestyle='--', alpha=0.5, label='50%')\n",
    "    ax.axvline(metrics['l50'], color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Number of Contigs (log scale)')\n",
    "    ax.set_ylabel('Cumulative Length (log scale)')\n",
    "    ax.set_title(f\"L50 = {metrics['l50']}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3, which='both')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_contig_distribution(metrics1, \"Assembly 1: Balanced\")\n",
    "plot_contig_distribution(metrics2, \"Assembly 2: Few Large + Many Small\")\n",
    "plot_contig_distribution(metrics3, \"Assembly 3: Fragmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b835d64",
   "metadata": {},
   "source": [
    "## 3. Accuracy Assessment (Reference-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_alignment_identity(query: str, subject: str, window: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Simple sequence identity calculation (not a full alignment).\n",
    "    Uses sliding window approach.\n",
    "    \"\"\"\n",
    "    if len(query) == 0 or len(subject) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(min(len(query), len(subject))):\n",
    "        if query[i] == subject[i]:\n",
    "            matches += 1\n",
    "        total += 1\n",
    "    \n",
    "    return (matches / total * 100) if total > 0 else 0.0\n",
    "\n",
    "def assess_accuracy(contigs: List[str], reference: str, \n",
    "                   coverage_map: Dict = None) -> dict:\n",
    "    \"\"\"\n",
    "    Assess assembly accuracy against a reference genome.\n",
    "    \n",
    "    Metrics:\n",
    "    - Sequence identity: % of bases matching reference\n",
    "    - Assembly size vs reference size\n",
    "    - Structural mismatches: inversions, duplications, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    assembled_length = sum(len(c) for c in contigs)\n",
    "    reference_length = len(reference)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    reference_positions_covered = set()\n",
    "    for contig in contigs:\n",
    "        for i in range(len(reference) - len(contig) + 1):\n",
    "            if reference[i:i+len(contig)] == contig:\n",
    "                reference_positions_covered.update(range(i, i+len(contig)))\n",
    "    \n",
    "    completeness = len(reference_positions_covered) / reference_length * 100\n",
    "    \n",
    "    # Calculate identity\n",
    "    assembled_seq = ''.join(contigs)\n",
    "    identity = simple_alignment_identity(assembled_seq, reference)\n",
    "    \n",
    "    return {\n",
    "        'assembled_length': assembled_length,\n",
    "        'reference_length': reference_length,\n",
    "        'length_ratio': assembled_length / reference_length,\n",
    "        'completeness_percent': completeness,\n",
    "        'sequence_identity': identity,\n",
    "        'assembly_contigs': len(contigs),\n",
    "    }\n",
    "\n",
    "# Test accuracy assessment\n",
    "print(\"\\nAccuracy Assessment (Reference-Based)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ref_seq = \"ATGCGATCGATCGATCGATCGATCGATCGATCG\" * 50  # 1700 bp reference\n",
    "\n",
    "# Perfect assembly\n",
    "perfect_assembly = [ref_seq]\n",
    "\n",
    "# Assembly with errors\n",
    "error_assembly = [ref_seq[:500], ref_seq[600:1000], ref_seq[1100:]]  # Missing regions\n",
    "\n",
    "# Assembly with sequence errors\n",
    "error_seq = list(ref_seq)\n",
    "for i in [100, 200, 500, 800]:  # Introduce errors\n",
    "    if i < len(error_seq):\n",
    "        bases = ['A', 'T', 'G', 'C']\n",
    "        error_seq[i] = [b for b in bases if b != error_seq[i]][0]\n",
    "identity_assembly = [''.join(error_seq)]\n",
    "\n",
    "accuracy_perfect = assess_accuracy(perfect_assembly, ref_seq)\n",
    "accuracy_gaps = assess_accuracy(error_assembly, ref_seq)\n",
    "accuracy_errors = assess_accuracy(identity_assembly, ref_seq)\n",
    "\n",
    "for name, accuracy in [(\"Perfect Assembly\", accuracy_perfect),\n",
    "                      (\"Assembly with Gaps\", accuracy_gaps),\n",
    "                      (\"Assembly with Errors\", accuracy_errors)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Assembled vs Reference: {accuracy['assembled_length']:,}/{accuracy['reference_length']:,} bp ({accuracy['length_ratio']*100:.1f}%)\")\n",
    "    print(f\"  Completeness: {accuracy['completeness_percent']:.1f}% of reference covered\")\n",
    "    print(f\"  Sequence Identity: {accuracy['sequence_identity']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981b3a56",
   "metadata": {},
   "source": [
    "## 4. Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reads_to_contigs(reads: List[str], contigs: List[str], \n",
    "                        min_match: int = 20) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Simple read mapping to contigs.\n",
    "    Returns dict mapping contig_id -> list of coverage at each position.\n",
    "    \"\"\"\n",
    "    coverage_maps = {}\n",
    "    \n",
    "    for c_idx, contig in enumerate(contigs):\n",
    "        coverage = [0] * len(contig)\n",
    "        \n",
    "        for read in reads:\n",
    "            # Try to find where read maps\n",
    "            for i in range(len(contig) - min_match + 1):\n",
    "                if contig[i:].startswith(read[:min_match]):\n",
    "                    # Found a match\n",
    "                    for j in range(min(len(read), len(contig) - i)):\n",
    "                        coverage[i + j] += 1\n",
    "                    break\n",
    "        \n",
    "        coverage_maps[c_idx] = coverage\n",
    "    \n",
    "    return coverage_maps\n",
    "\n",
    "def analyze_coverage(coverage_maps: Dict[int, List[int]]) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze coverage distribution.\n",
    "    \"\"\"\n",
    "    all_coverage = []\n",
    "    for contig_coverage in coverage_maps.values():\n",
    "        all_coverage.extend([c for c in contig_coverage if c > 0])\n",
    "    \n",
    "    if not all_coverage:\n",
    "        return {'mean': 0, 'median': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(all_coverage),\n",
    "        'median': np.median(all_coverage),\n",
    "        'std': np.std(all_coverage),\n",
    "        'min': np.min(all_coverage),\n",
    "        'max': np.max(all_coverage),\n",
    "        'cv': np.std(all_coverage) / np.mean(all_coverage) if np.mean(all_coverage) > 0 else 0\n",
    "    }\n",
    "\n",
    "def plot_coverage(coverage_maps: Dict[int, List[int]], title: str = \"Read Coverage Profile\"):\n",
    "    \"\"\"\n",
    "    Plot coverage for all contigs.\n",
    "    \"\"\"\n",
    "    n_contigs = len(coverage_maps)\n",
    "    \n",
    "    if n_contigs == 1:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "        axes = [ax]\n",
    "    elif n_contigs <= 4:\n",
    "        fig, axes = plt.subplots(n_contigs, 1, figsize=(12, 3*n_contigs))\n",
    "        if n_contigs == 1:\n",
    "            axes = [axes]\n",
    "    else:\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(12, 12))\n",
    "    \n",
    "    for idx, (c_id, coverage) in enumerate(sorted(coverage_maps.items())[:4]):\n",
    "        ax = axes[idx]\n",
    "        x = np.arange(len(coverage))\n",
    "        ax.fill_between(x, 0, coverage, color='steelblue', alpha=0.7)\n",
    "        ax.plot(x, coverage, color='darkblue', linewidth=1)\n",
    "        \n",
    "        if coverage:\n",
    "            mean_cov = np.mean([c for c in coverage if c > 0])\n",
    "            ax.axhline(mean_cov, color='red', linestyle='--', alpha=0.5, label=f'Mean={mean_cov:.1f}x')\n",
    "        \n",
    "        ax.set_ylabel('Coverage (x)')\n",
    "        ax.set_title(f'Contig {c_id} Coverage')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test coverage analysis\n",
    "print(\"\\nCoverage Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_contigs = [\n",
    "    \"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\",\n",
    "    \"TCGACGTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTA\"\n",
    "]\n",
    "\n",
    "# Generate reads with variable coverage\n",
    "import random\n",
    "random.seed(42)\n",
    "test_reads = []\n",
    "for _ in range(50):\n",
    "    contig = random.choice(test_contigs)\n",
    "    start = random.randint(0, max(0, len(contig) - 30))\n",
    "    test_reads.append(contig[start:start+30])\n",
    "\n",
    "cov_maps = map_reads_to_contigs(test_reads, test_contigs, min_match=15)\n",
    "cov_analysis = analyze_coverage(cov_maps)\n",
    "\n",
    "print(f\"Coverage Statistics:\")\n",
    "print(f\"  Mean: {cov_analysis['mean']:.2f}x\")\n",
    "print(f\"  Median: {cov_analysis['median']:.2f}x\")\n",
    "print(f\"  Std Dev: {cov_analysis['std']:.2f}x\")\n",
    "print(f\"  Min/Max: {cov_analysis['min']:.0f}x / {cov_analysis['max']:.0f}x\")\n",
    "print(f\"  Coefficient of Variation: {cov_analysis['cv']:.3f}\")\n",
    "\n",
    "if cov_analysis['cv'] < 0.2:\n",
    "    print(\"  âœ“ Coverage is uniform (good)\")\n",
    "elif cov_analysis['cv'] < 0.5:\n",
    "    print(\"  ~ Coverage is moderately variable\")\n",
    "else:\n",
    "    print(\"  âœ— Coverage is highly variable (potential problems)\")\n",
    "\n",
    "plot_coverage(cov_maps, \"Test Assembly Coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824df93",
   "metadata": {},
   "source": [
    "## 5. Repeat Resolution Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71517ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_repeats_in_assembly(contigs: List[str], min_length: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Detect potential repeat sequences and duplications in the assembly.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Find all k-mers across contigs\n",
    "    k = 31  # Use relatively large k to be specific\n",
    "    kmer_counts = Counter()\n",
    "    kmer_locations = defaultdict(list)  # kmer -> list of (contig_id, position)\n",
    "    \n",
    "    for c_idx, contig in enumerate(contigs):\n",
    "        for i in range(len(contig) - k + 1):\n",
    "            kmer = contig[i:i+k]\n",
    "            kmer_counts[kmer] += 1\n",
    "            kmer_locations[kmer].append((c_idx, i))\n",
    "    \n",
    "    # Find k-mers that appear multiple times\n",
    "    repeated_kmers = {kmer: count for kmer, count in kmer_counts.items() if count > 1}\n",
    "    \n",
    "    # Find contigs with high repeat content\n",
    "    repeat_content = {}\n",
    "    for c_idx, contig in enumerate(contigs):\n",
    "        repeat_bases = 0\n",
    "        for i in range(len(contig) - k + 1):\n",
    "            kmer = contig[i:i+k]\n",
    "            if kmer_counts[kmer] > 1:\n",
    "                repeat_bases += 1\n",
    "        repeat_content[c_idx] = (repeat_bases * k) / len(contig) * 100 if contig else 0\n",
    "    \n",
    "    return {\n",
    "        'repeated_kmers': len(repeated_kmers),\n",
    "        'max_repeat_count': max(repeated_kmers.values()) if repeated_kmers else 0,\n",
    "        'repeat_content': repeat_content,\n",
    "        'high_repeat_contigs': [c for c, pct in repeat_content.items() if pct > 30]\n",
    "    }\n",
    "\n",
    "print(\"\\nRepeat Resolution Assessment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test assembly with repeats\n",
    "repeat_unit = \"ATGCGATCGATCGATCGATCG\"\n",
    "unique1 = \"TTTTAAAA\"\n",
    "unique2 = \"GGGGCCCC\"\n",
    "\n",
    "contig_with_repeat = unique1 + repeat_unit * 3 + unique2  # Contains repeat\n",
    "contig_unique = \"AAATTTGGGCCCAAATTTGGG\" * 5\n",
    "\n",
    "test_contigs_repeat = [contig_with_repeat, contig_unique]\n",
    "\n",
    "repeat_analysis = detect_repeats_in_assembly(test_contigs_repeat)\n",
    "\n",
    "print(f\"Repeat Analysis:\")\n",
    "print(f\"  Repeated 31-mers found: {repeat_analysis['repeated_kmers']}\")\n",
    "print(f\"  Max repeat count: {repeat_analysis['max_repeat_count']}x\")\n",
    "print(f\"\\nRepeat content by contig:\")\n",
    "for c_idx, pct in repeat_analysis['repeat_content'].items():\n",
    "    print(f\"  Contig {c_idx}: {pct:.1f}% repeat-derived bases\")\n",
    "    if pct > 30:\n",
    "        print(f\"             âš  High repeat content (assembly may be fragmented)\")\n",
    "\n",
    "print(f\"\\nContigs with high repeat content: {repeat_analysis['high_repeat_contigs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c49e95",
   "metadata": {},
   "source": [
    "## 6. Comparing Multiple Assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_assemblies(assemblies: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compare multiple assemblies comprehensively.\n",
    "    assemblies: dict mapping assembly_name -> list of contigs\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, contigs in assemblies.items():\n",
    "        lengths = [len(c) for c in contigs]\n",
    "        metrics = calculate_contiguity_metrics(lengths)\n",
    "        \n",
    "        results[name] = {\n",
    "            'num_contigs': metrics['num_contigs'],\n",
    "            'total_length': metrics['total_length'],\n",
    "            'n50': metrics['n50'],\n",
    "            'l50': metrics['l50'],\n",
    "            'mean_length': metrics['mean_length'],\n",
    "            'max_length': metrics['max_length']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_comparison_table(comparisons: dict):\n",
    "    \"\"\"\n",
    "    Print comparison of assemblies.\n",
    "    \"\"\"\n",
    "    print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                        ASSEMBLY COMPARISON                                           â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "{'Assembly':<20} {'Contigs':<10} {'Total BP':<15} {'N50':<12} {'L50':<8} {'Max':<12} {'Mean':<10}\n",
    "{'â”€'*95}\n",
    "\"\"\")\n",
    "    \n",
    "    for name in sorted(comparisons.keys()):\n",
    "        c = comparisons[name]\n",
    "        print(f\"{name:<20} {c['num_contigs']:<10} {c['total_length']:<15,} {c['n50']:<12,} {c['l50']:<8} {c['max_length']:<12,} {c['mean_length']:<10.0f}\")\n",
    "\n",
    "print(\"\\nAssembly Comparison Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create three different assemblies\n",
    "assembly_a = test_assembly1  # Balanced\n",
    "assembly_b = test_assembly2  # Few large + many small\n",
    "assembly_c = test_assembly3  # Fragmented\n",
    "\n",
    "assemblies = {\n",
    "    \"Assembly A (Balanced)\": [[str(i)] * l for l in assembly_a],\n",
    "    \"Assembly B (Few Large)\": [[str(i)] * l for l in assembly_b],\n",
    "    \"Assembly C (Fragmented)\": [[str(i)] * l for l in assembly_c]\n",
    "}\n",
    "\n",
    "comps = compare_assemblies(assemblies)\n",
    "print_comparison_table(comps)\n",
    "\n",
    "print(\"\\nRanking by Key Metrics:\")\n",
    "print(f\"\\nBest N50:\")\n",
    "best_n50 = max(comps.items(), key=lambda x: x[1]['n50'])\n",
    "print(f\"  {best_n50[0]}: {best_n50[1]['n50']} bp\")\n",
    "\n",
    "print(f\"\\nBest contiguity (lowest L50):\")\n",
    "best_l50 = min(comps.items(), key=lambda x: x[1]['l50'])\n",
    "print(f\"  {best_l50[0]}: {best_l50[1]['l50']} contigs\")\n",
    "\n",
    "print(f\"\\nLongest single contig:\")\n",
    "best_max = max(comps.items(), key=lambda x: x[1]['max_length'])\n",
    "print(f\"  {best_max[0]}: {best_max[1]['max_length']} bp\")\n",
    "\n",
    "print(f\"\\nğŸ† Overall best: {best_n50[0]} (highest N50 is most important)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb886a4e",
   "metadata": {},
   "source": [
    "## 7. Choosing an Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_guide = \"\"\"\n",
    "CHOOSING THE RIGHT ASSEMBLER\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€ SHORT READS (Illumina, up to ~250bp) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  SPADES (SPAdes - Genome Assembler for SPecial PurposEs) - MOST POPULAR            â”‚\n",
    "â”‚  â€¢ De Bruijn graph based                                                           â”‚\n",
    "â”‚  â€¢ Uses multiple k-mer sizes (e.g., 21, 33, 55, 77)                               â”‚\n",
    "â”‚  â€¢ Excellent for bacteria, fungi, small eukaryotes                                â”‚\n",
    "â”‚  â€¢ Good error correction                                                           â”‚\n",
    "â”‚  â€¢ CPU: ~10-50GB RAM, reasonable time                                              â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda spades                                       â”‚\n",
    "â”‚  â€¢ Use: spades.py -1 reads_R1.fastq -2 reads_R2.fastq -o output_dir               â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  VELVET                                                                             â”‚\n",
    "â”‚  â€¢ Original De Bruijn graph assembler                                              â”‚\n",
    "â”‚  â€¢ Simple, fast, but needs tuning                                                  â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda velvet                                       â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  ABYSS                                                                              â”‚\n",
    "â”‚  â€¢ De Bruijn graph, parallelizable                                                 â”‚\n",
    "â”‚  â€¢ Good for large genomes                                                          â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda abyss                                        â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€ LONG READS (PacBio, Oxford Nanopore, >10kb) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  FLYE                                                                               â”‚\n",
    "â”‚  â€¢ De Bruijn-like for long reads                                                   â”‚\n",
    "â”‚  â€¢ Fast, accurate                                                                  â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda flye                                         â”‚\n",
    "â”‚  â€¢ Use: flye --pacbio-raw reads.fastq -o output_dir -t 32                         â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  CANU                                                                               â”‚\n",
    "â”‚  â€¢ Overlap-Layout-Consensus                                                        â”‚\n",
    "â”‚  â€¢ Accurate even with 5-10% error rate                                             â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda canu                                         â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€ HYBRID (Mix of short + long reads) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  SPADES (with long reads)                                                           â”‚\n",
    "â”‚  â€¢ Use: spades.py -1 short_R1.fastq -2 short_R2.fastq --pacbio long_reads.fastq    â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â”‚  UNICYCLER                                                                          â”‚\n",
    "â”‚  â€¢ Hybrid assembler specifically designed for hybrid assemblies                    â”‚\n",
    "â”‚  â€¢ Install: conda install -c bioconda unicycler                                    â”‚\n",
    "â”‚                                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "SELECTING BY ORGANISM\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Bacteria / Archaea (typically 2-10 Mb, haploid):\n",
    "  â†’ SPAdes (default choice)\n",
    "  â†’ For high quality: FLYE if long reads available\n",
    "  â†’ For very large (>50 Mb): Consider MEGAHIT\n",
    "\n",
    "Fungi (typically 10-100 Mb, often haploid):\n",
    "  â†’ SPAdes\n",
    "  â†’ FLYE for long reads\n",
    "\n",
    "Plants (typically 100-10,000 Mb, diploid/polyploid):\n",
    "  â†’ FLYE (long reads preferred)\n",
    "  â†’ SPAdes for short reads only (may struggle with large genomes)\n",
    "  â†’ Consider: PLATANUS, ALLPATHS-LG\n",
    "\n",
    "Animals (typically 100-10,000 Mb):\n",
    "  â†’ FLYE or Canu (long reads)\n",
    "  â†’ SPAdes (short reads, for smaller animals)\n",
    "  â†’ Hybrid: UNICYCLER, PLATANUS\n",
    "\n",
    "Viruses (typically <500 kb):\n",
    "  â†’ Any assembler works\n",
    "  â†’ SPAdes is common choice\n",
    "\n",
    "KEY QUALITY THRESHOLDS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Bacterial assembly quality expectations:\n",
    "  N50 > 50 kb    : Good\n",
    "  N50 > 200 kb   : Very Good\n",
    "  N50 > 1 Mb     : Excellent (near complete)\n",
    "\n",
    "Eukaryotic assembly quality (human-sized):\n",
    "  N50 > 100 kb   : Reasonable (can improve scaffolding)\n",
    "  N50 > 1 Mb     : Good\n",
    "  N50 > 10 Mb    : Very Good\n",
    "  N50 > 50 Mb    : Excellent\n",
    "\n",
    "READ COVERAGE RECOMMENDATIONS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Sequencing depth (coverage) needed:\n",
    "  Bacteria:      10-20x sufficient\n",
    "  Fungi:         20-50x recommended\n",
    "  Plants:        30-50x (diploid), 50-100x (polyploid)\n",
    "  Vertebrates:   30-50x typical\n",
    "  Complex:       50-100x for confident assembly\n",
    "\n",
    "Higher coverage helps with:\n",
    "  â€¢ Error correction\n",
    "  â€¢ Resolving repeats\n",
    "  â€¢ Low-coverage regions\n",
    "  â€¢ Polyploidy\n",
    "\n",
    "Diminishing returns above 100x for most simple genomes.\n",
    "\"\"\"\n",
    "\n",
    "print(assembler_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbd4c1",
   "metadata": {},
   "source": [
    "## 8. Quality Assessment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_report(assembly: List[str], reference: str = None, reads: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive quality assessment report.\n",
    "    \"\"\"\n",
    "    lengths = [len(c) for c in assembly]\n",
    "    metrics = calculate_contiguity_metrics(lengths)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘            ASSEMBLY QUALITY ASSESSMENT REPORT                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "[âœ“] CONTIGUITY\n",
    "  â€¢ Number of contigs: {metrics['num_contigs']}\n",
    "  â€¢ Total length: {metrics['total_length']:,} bp\n",
    "  â€¢ N50: {metrics['n50']} bp (L50={metrics['l50']})\n",
    "  â€¢ Longest contig: {metrics['max_length']} bp\n",
    "  â€¢ Mean length: {metrics['mean_length']:.0f} bp\n",
    "  â€¢ Status: \"\"\"\n",
    "    \n",
    "    if metrics['n50'] > 100000:\n",
    "        report += \"âœ“ EXCELLENT\\n\"\n",
    "    elif metrics['n50'] > 50000:\n",
    "        report += \"âœ“ GOOD\\n\"\n",
    "    elif metrics['n50'] > 10000:\n",
    "        report += \"~ FAIR\\n\"\n",
    "    else:\n",
    "        report += \"âœ— FRAGMENTED\\n\"\n",
    "    \n",
    "    # Repeat analysis\n",
    "    repeat_analysis = detect_repeats_in_assembly(assembly)\n",
    "    report += f\"\"\"\n",
    "[?] REPEAT CONTENT\n",
    "  â€¢ Repeated k-mers: {repeat_analysis['repeated_kmers']}\n",
    "  â€¢ Max repeat frequency: {repeat_analysis['max_repeat_count']}x\n",
    "  â€¢ High-repeat contigs: {len(repeat_analysis['high_repeat_contigs'])}\n",
    "  â€¢ Status: \"\"\"\n",
    "    \n",
    "    if len(repeat_analysis['high_repeat_contigs']) == 0:\n",
    "        report += \"âœ“ Low repeat content\\n\"\n",
    "    else:\n",
    "        report += f\"~ Some contigs have high repeat content\\n\"\n",
    "    \n",
    "    if reference:\n",
    "        accuracy = assess_accuracy(assembly, reference)\n",
    "        report += f\"\"\"\n",
    "[R] ACCURACY (vs reference)\n",
    "  â€¢ Assembled length: {accuracy['assembled_length']:,} bp\n",
    "  â€¢ Reference length: {accuracy['reference_length']:,} bp\n",
    "  â€¢ Coverage: {accuracy['length_ratio']*100:.1f}% of reference\n",
    "  â€¢ Completeness: {accuracy['completeness_percent']:.1f}%\n",
    "  â€¢ Sequence identity: {accuracy['sequence_identity']:.2f}%\n",
    "  â€¢ Status: \"\"\"\n",
    "        \n",
    "        if accuracy['completeness_percent'] > 95:\n",
    "            report += \"âœ“ Complete\\n\"\n",
    "        elif accuracy['completeness_percent'] > 80:\n",
    "            report += \"âœ“ Good coverage\\n\"\n",
    "        else:\n",
    "            report += \"~ Incomplete\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "[\"] RECOMMENDATIONS\n",
    "  â€¢ For publication/production use:\n",
    "    - Validate contigs with independent data (PCR, qPCR, other sequencing)\n",
    "    - Fill gaps where possible\n",
    "    - Polish with long reads if available\n",
    "    - Check for chimerism (correct assembly errors)\n",
    "    \n",
    "  â€¢ To improve assembly:\n",
    "\"\"\"\n",
    "    \n",
    "    if metrics['n50'] < 50000:\n",
    "        report += \"    - Try longer reads or higher coverage\\n\"\n",
    "    if len(repeat_analysis['high_repeat_contigs']) > 0:\n",
    "        report += \"    - Use longer reads to resolve repeat regions\\n\"\n",
    "    if reference and accuracy['completeness_percent'] < 95:\n",
    "        report += \"    - Increase sequencing coverage\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate sample report\n",
    "test_assembly = [test_assembly1[0], test_assembly1[1], test_assembly1[2]]  # Take a few contigs\n",
    "report = generate_qa_report(test_assembly[:3])\n",
    "print(\"\\n\" + report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45c4a8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Assembly Quality Metrics:**\n",
    "- **Contiguity**: N50/L50 most important; higher is better\n",
    "- **Completeness**: What % of the genome is covered?\n",
    "- **Accuracy**: Sequence identity to reference (if available)\n",
    "- **Coverage**: Is coverage uniform? Indicates assembly problems if not\n",
    "- **Repeats**: High repeat content â†’ fragmented assembly\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. No single metric is perfectâ€”use all together\n",
    "2. N50 is most commonly reported but context matters\n",
    "3. Compare multiple assemblies if possible\n",
    "4. Consider your organism and goals when evaluating\n",
    "5. Validation against independent data is essential for production genomes\n",
    "\n",
    "**Tools for Quality Assessment:**\n",
    "- `QUAST`: Automated QA if reference is available\n",
    "- `Busco`: Completeness check against conserved genes\n",
    "- `AssemblyStats`: Simple statistics\n",
    "- Custom scripts (like this notebook!)\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the genome assembly series! You now understand:\n",
    "1. âœ“ Fundamentals (k-mers, De Bruijn graphs)\n",
    "2. âœ“ Read preprocessing (QC, trimming, filtering)\n",
    "3. âœ“ Assembly algorithms (greedy vs De Bruijn)\n",
    "4. âœ“ Scaffolding and contig joining\n",
    "5. âœ“ Quality assessment\n",
    "\n",
    "Next steps: Try it yourself with real data from SRA!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
