{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f06339",
   "metadata": {},
   "source": [
    "# Genome Assembly: 2. Read Preprocessing & Quality Control\n",
    "\n",
    "## Overview\n",
    "Before assembly, raw sequencing data must be cleaned:\n",
    "- Quality assessment and visualization\n",
    "- Adapter trimming (sequences added by the sequencer)\n",
    "- Quality-based trimming and filtering\n",
    "- Complexity filtering (low-entropy reads)\n",
    "- Duplicate detection\n",
    "\n",
    "**Data Source**: We'll use realistic simulated FASTQ data (you can replace with real data from:\n",
    "- SRA (Sequence Read Archive): https://www.ncbi.nlm.nih.gov/sra\n",
    "- ENA (European Nucleotide Archive): https://www.ebi.ac.uk/ena)\n",
    "\n",
    "**Goal**: Transform raw reads into high-quality input for assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95d6b7",
   "metadata": {},
   "source": [
    "## 1. FASTQ Format and Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af16877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Iterator\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "class FASTQRecord:\n",
    "    \"\"\"A single FASTQ record (sequencing read)\"\"\"\n",
    "    def __init__(self, header: str, sequence: str, plus: str, qualities: str):\n",
    "        self.header = header\n",
    "        self.sequence = sequence\n",
    "        self.plus = plus\n",
    "        self.qualities = qualities\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"FASTQRecord(id='{self.header[:30]}...', len={len(self.sequence)}bp)\"\n",
    "    \n",
    "    def get_phred_scores(self) -> List[int]:\n",
    "        \"\"\"Convert quality characters to Phred scores\"\"\"\n",
    "        # Standard Illumina uses ASCII offset 33 (Phred+33)\n",
    "        return [ord(c) - 33 for c in self.qualities]\n",
    "    \n",
    "    def mean_quality(self) -> float:\n",
    "        \"\"\"Calculate mean quality score\"\"\"\n",
    "        return np.mean(self.get_phred_scores())\n",
    "    \n",
    "    def min_quality(self) -> int:\n",
    "        \"\"\"Get minimum quality score\"\"\"\n",
    "        return min(self.get_phred_scores())\n",
    "\n",
    "def parse_fastq(file_path: str) -> Iterator[FASTQRecord]:\n",
    "    \"\"\"\n",
    "    Parse a FASTQ file (gzip-compressed or plain text).\n",
    "    Yields FASTQRecord objects.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if file_path.suffix == '.gz':\n",
    "        opener = gzip.open\n",
    "    else:\n",
    "        opener = open\n",
    "    \n",
    "    with opener(file_path, 'rt') as f:\n",
    "        while True:\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break\n",
    "            \n",
    "            sequence = f.readline().strip()\n",
    "            plus = f.readline().strip()\n",
    "            qualities = f.readline().strip()\n",
    "            \n",
    "            yield FASTQRecord(header, sequence, plus, qualities)\n",
    "\n",
    "# Example FASTQ content\n",
    "fastq_example = \"\"\"@read1/1\n",
    "ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\n",
    "+\n",
    "IIIIIIIIIIIIIIIIIIIIIIIIIIIIHHHHHHHHFFFFFFFFDDDDDDDD\n",
    "@read2/1\n",
    "ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\n",
    "+\n",
    "IIIIIIIIIIIIHHHHHHHHHHHHHHHHFFFFFFFFFFDDDDDDDD999999\"\"\"\n",
    "\n",
    "print(\"FASTQ Format Example:\")\n",
    "print(fastq_example)\n",
    "print(f\"\\nLine 1: @header - read identifier\")\n",
    "print(f\"Line 2: sequence - DNA bases\")\n",
    "print(f\"Line 3: + - separator (sometimes repeated header)\")\n",
    "print(f\"Line 4: quality - ASCII characters encoding Phred scores\")\n",
    "\n",
    "# Save example for testing\n",
    "test_fastq_path = \"test_reads.fastq\"\n",
    "with open(test_fastq_path, 'w') as f:\n",
    "    f.write(fastq_example)\n",
    "\n",
    "print(f\"\\nTest FASTQ file created at: {test_fastq_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab28b8",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Realistic Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def phred_to_char(q_score: int) -> str:\n",
    "    \"\"\"Convert Phred score to quality character (Phred+33)\"\"\"\n",
    "    return chr(q_score + 33)\n",
    "\n",
    "def generate_synthetic_reads(reference: str, num_reads: int = 1000, read_length: int = 100, \n",
    "                              coverage: int = 10, error_rate: float = 0.01) -> List[FASTQRecord]:\n",
    "    \"\"\"\n",
    "    Generate synthetic reads from a reference genome with realistic errors.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference genome sequence\n",
    "        num_reads: Number of reads to generate\n",
    "        read_length: Length of each read\n",
    "        coverage: Expected coverage depth\n",
    "        error_rate: Probability of each base being erroneous\n",
    "    \"\"\"\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    reads = []\n",
    "    \n",
    "    for i in range(num_reads):\n",
    "        # Random position in reference\n",
    "        start = random.randint(0, len(reference) - read_length)\n",
    "        read_seq = reference[start:start + read_length]\n",
    "        \n",
    "        # Introduce errors\n",
    "        read_list = list(read_seq)\n",
    "        for j in range(len(read_list)):\n",
    "            if random.random() < error_rate:\n",
    "                # Substitute with random base\n",
    "                read_list[j] = random.choice([b for b in bases if b != read_list[j]])\n",
    "        read_seq = ''.join(read_list)\n",
    "        \n",
    "        # Generate quality scores\n",
    "        # High quality at read start/end, lower in middle (realistic pattern)\n",
    "        qualities = []\n",
    "        for j in range(read_length):\n",
    "            # Quality decreases towards the end\n",
    "            position_factor = j / read_length\n",
    "            base_quality = int(35 - 10 * position_factor)\n",
    "            # Add some randomness\n",
    "            q = max(5, base_quality + random.randint(-3, 3))\n",
    "            # Lower quality at error positions\n",
    "            if read_list[j] != read_seq[j]:\n",
    "                q = min(q, random.randint(5, 15))\n",
    "            qualities.append(phred_to_char(min(40, q)))\n",
    "        \n",
    "        header = f\"@read{i}/1\"\n",
    "        plus = \"+\"\n",
    "        reads.append(FASTQRecord(header, read_seq, plus, ''.join(qualities)))\n",
    "    \n",
    "    return reads\n",
    "\n",
    "# Generate realistic genome and reads\n",
    "print(\"Generating synthetic bacterial genome...\")\n",
    "# Simple genome with some repeats\n",
    "genome_base = \"ATGCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCG\"\n",
    "genome = genome_base * 50  # ~2250 bp\n",
    "print(f\"Genome length: {len(genome)} bp\")\n",
    "\n",
    "print(\"\\nGenerating 5000 reads with 1% error rate...\")\n",
    "synthetic_reads = generate_synthetic_reads(genome, num_reads=5000, read_length=100, error_rate=0.01)\n",
    "print(f\"Generated {len(synthetic_reads)} reads\")\n",
    "print(f\"Example read: {synthetic_reads[0]}\")\n",
    "print(f\"  Sequence: {synthetic_reads[0].sequence[:50]}...\")\n",
    "print(f\"  Quality:  {synthetic_reads[0].qualities[:50]}...\")\n",
    "print(f\"  Mean Q:   {synthetic_reads[0].mean_quality():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f914a",
   "metadata": {},
   "source": [
    "## 3. Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_quality(reads: List[FASTQRecord]) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate quality statistics across all reads.\n",
    "    \"\"\"\n",
    "    read_lengths = [len(r.sequence) for r in reads]\n",
    "    mean_qualities = [r.mean_quality() for r in reads]\n",
    "    min_qualities = [r.min_quality() for r in reads]\n",
    "    gc_contents = []\n",
    "    \n",
    "    for read in reads:\n",
    "        gc = (read.sequence.count('G') + read.sequence.count('C')) / len(read.sequence)\n",
    "        gc_contents.append(gc)\n",
    "    \n",
    "    return {\n",
    "        'num_reads': len(reads),\n",
    "        'total_bases': sum(read_lengths),\n",
    "        'read_length_mean': np.mean(read_lengths),\n",
    "        'read_length_median': np.median(read_lengths),\n",
    "        'mean_quality_mean': np.mean(mean_qualities),\n",
    "        'mean_quality_min': np.min(mean_qualities),\n",
    "        'mean_quality_max': np.max(mean_qualities),\n",
    "        'min_quality_min': np.min(min_qualities),\n",
    "        'min_quality_max': np.max(min_qualities),\n",
    "        'gc_mean': np.mean(gc_contents),\n",
    "        'gc_std': np.std(gc_contents),\n",
    "        'mean_qualities': mean_qualities,\n",
    "        'min_qualities': min_qualities,\n",
    "        'gc_contents': gc_contents,\n",
    "        'quality_by_position': quality_by_position(reads)\n",
    "    }\n",
    "\n",
    "def quality_by_position(reads: List[FASTQRecord]) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate quality score statistics at each position across all reads.\n",
    "    \"\"\"\n",
    "    if not reads:\n",
    "        return {}\n",
    "    \n",
    "    read_length = len(reads[0].sequence)\n",
    "    position_qualities = defaultdict(list)\n",
    "    \n",
    "    for read in reads:\n",
    "        phred_scores = read.get_phred_scores()\n",
    "        for pos, q in enumerate(phred_scores):\n",
    "            position_qualities[pos].append(q)\n",
    "    \n",
    "    result = {}\n",
    "    for pos in range(read_length):\n",
    "        qs = position_qualities[pos]\n",
    "        result[pos] = {\n",
    "            'mean': np.mean(qs),\n",
    "            'median': np.median(qs),\n",
    "            'std': np.std(qs)\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Analyze\n",
    "stats = analyze_quality(synthetic_reads)\n",
    "\n",
    "print(\"=== QUALITY ASSESSMENT ===\")\n",
    "print(f\"Total reads: {stats['num_reads']:,}\")\n",
    "print(f\"Total bases: {stats['total_bases']:,}\")\n",
    "print(f\"\\nRead length:\")\n",
    "print(f\"  Mean: {stats['read_length_mean']:.1f} bp\")\n",
    "print(f\"  Median: {stats['read_length_median']:.1f} bp\")\n",
    "print(f\"\\nMean quality per read:\")\n",
    "print(f\"  Mean: {stats['mean_quality_mean']:.1f}\")\n",
    "print(f\"  Min: {stats['mean_quality_min']:.1f}\")\n",
    "print(f\"  Max: {stats['mean_quality_max']:.1f}\")\n",
    "print(f\"\\nMinimum quality per read:\")\n",
    "print(f\"  Min: {stats['min_quality_min']}\")\n",
    "print(f\"  Max: {stats['min_quality_max']}\")\n",
    "print(f\"\\nGC content:\")\n",
    "print(f\"  Mean: {stats['gc_mean']:.2%}\")\n",
    "print(f\"  Std:  {stats['gc_std']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cdaa2d",
   "metadata": {},
   "source": [
    "## 4. Quality Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 1. Mean quality distribution\n",
    "axes[0, 0].hist(stats['mean_qualities'], bins=30, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(stats['mean_qualities']), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].set_xlabel('Mean Quality Score')\n",
    "axes[0, 0].set_ylabel('Number of Reads')\n",
    "axes[0, 0].set_title('Distribution of Mean Quality per Read')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Quality by position\n",
    "positions = sorted(stats['quality_by_position'].keys())\n",
    "means = [stats['quality_by_position'][p]['mean'] for p in positions]\n",
    "stds = [stats['quality_by_position'][p]['std'] for p in positions]\n",
    "\n",
    "axes[0, 1].plot(positions, means, color='steelblue', linewidth=2, label='Mean')\n",
    "axes[0, 1].fill_between(positions, \n",
    "                         np.array(means) - np.array(stds),\n",
    "                         np.array(means) + np.array(stds),\n",
    "                         alpha=0.3, color='steelblue', label='±1 Std')\n",
    "axes[0, 1].axhline(30, color='green', linestyle='--', alpha=0.5, label='Q30 threshold')\n",
    "axes[0, 1].axhline(20, color='orange', linestyle='--', alpha=0.5, label='Q20 threshold')\n",
    "axes[0, 1].set_xlabel('Position in Read (bp)')\n",
    "axes[0, 1].set_ylabel('Quality Score')\n",
    "axes[0, 1].set_title('Quality Score Profile by Read Position')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 42])\n",
    "\n",
    "# 3. GC content distribution\n",
    "axes[1, 0].hist(stats['gc_contents'], bins=30, color='coral', edgecolor='black')\n",
    "axes[1, 0].axvline(np.mean(stats['gc_contents']), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 0].set_xlabel('GC Content')\n",
    "axes[1, 0].set_ylabel('Number of Reads')\n",
    "axes[1, 0].set_title('GC Content Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Min quality distribution\n",
    "axes[1, 1].hist(stats['min_qualities'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].axvline(np.mean(stats['min_qualities']), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 1].set_xlabel('Minimum Quality Score in Read')\n",
    "axes[1, 1].set_ylabel('Number of Reads')\n",
    "axes[1, 1].set_title('Distribution of Minimum Quality per Read')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Quality assessment plots displayed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdd8e2",
   "metadata": {},
   "source": [
    "## 5. Adapter Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48625ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adapter_end(sequence: str, adapter: str, min_overlap: int = 8, max_mismatch: float = 0.1) -> int:\n",
    "    \"\"\"\n",
    "    Find where an adapter sequence begins in the read.\n",
    "    Uses a simple approach: scan for the adapter allowing a few mismatches.\n",
    "    \n",
    "    Returns the position where the adapter starts, or len(sequence) if not found.\n",
    "    \"\"\"\n",
    "    for start in range(len(sequence) - min_overlap + 1):\n",
    "        # Check suffix of sequence starting at 'start' against adapter\n",
    "        suffix = sequence[start:]\n",
    "        # Allow partial matches at the very end\n",
    "        for adapter_start in range(len(adapter) - min_overlap + 1):\n",
    "            adapter_part = adapter[adapter_start:]\n",
    "            if len(adapter_part) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check how many bases match\n",
    "            match_length = min(len(suffix), len(adapter_part))\n",
    "            mismatches = sum(1 for i in range(match_length) if suffix[i] != adapter_part[i])\n",
    "            \n",
    "            if mismatches / match_length <= max_mismatch and match_length >= min_overlap:\n",
    "                return start\n",
    "    \n",
    "    return len(sequence)\n",
    "\n",
    "def trim_adapters(reads: List[FASTQRecord], adapter_sequences: dict = None) -> Tuple[List[FASTQRecord], dict]:\n",
    "    \"\"\"\n",
    "    Trim known adapter sequences from read ends.\n",
    "    Common Illumina adapters are included by default.\n",
    "    \"\"\"\n",
    "    if adapter_sequences is None:\n",
    "        # Common Illumina adapters\n",
    "        adapter_sequences = {\n",
    "            'Illumina_R1': 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCA',\n",
    "            'Illumina_R2': 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT',\n",
    "            'Truseq': 'GATCGGAAGAGC'\n",
    "        }\n",
    "    \n",
    "    trimmed_reads = []\n",
    "    trim_stats = {'total': 0, 'trimmed': 0, 'bases_removed': 0}\n",
    "    \n",
    "    for read in reads:\n",
    "        seq = read.sequence\n",
    "        qual = read.qualities\n",
    "        trim_pos = len(seq)  # Default: no trimming\n",
    "        \n",
    "        # Check each adapter\n",
    "        for adapter_name, adapter in adapter_sequences.items():\n",
    "            pos = find_adapter_end(seq, adapter)\n",
    "            trim_pos = min(trim_pos, pos)\n",
    "        \n",
    "        # Trim if adapter was found\n",
    "        if trim_pos < len(seq):\n",
    "            seq = seq[:trim_pos]\n",
    "            qual = qual[:trim_pos]\n",
    "            trim_stats['trimmed'] += 1\n",
    "            trim_stats['bases_removed'] += len(read.sequence) - trim_pos\n",
    "        \n",
    "        trimmed_reads.append(FASTQRecord(read.header, seq, read.plus, qual))\n",
    "        trim_stats['total'] += 1\n",
    "    \n",
    "    return trimmed_reads, trim_stats\n",
    "\n",
    "# Test with synthetic reads (add fake adapters)\n",
    "test_reads = synthetic_reads[:100].copy()\n",
    "# Add adapter to some reads\n",
    "for i in [10, 20, 30]:\n",
    "    r = test_reads[i]\n",
    "    adapter = 'AGATCGGAAGAGC'\n",
    "    test_reads[i] = FASTQRecord(r.header, r.sequence + adapter, r.plus, r.qualities + 'I' * len(adapter))\n",
    "\n",
    "trimmed, stats_trim = trim_adapters(test_reads)\n",
    "\n",
    "print(f\"Adapter Trimming Results:\")\n",
    "print(f\"  Total reads processed: {stats_trim['total']}\")\n",
    "print(f\"  Reads with adapters trimmed: {stats_trim['trimmed']}\")\n",
    "print(f\"  Total bases removed: {stats_trim['bases_removed']}\")\n",
    "print(f\"\\nExample:\")\n",
    "for i in [10, 20, 30]:\n",
    "    orig = test_reads[i]\n",
    "    trim = trimmed[i]\n",
    "    if len(orig.sequence) != len(trim.sequence):\n",
    "        print(f\"  Read {i}: {len(orig.sequence)}bp → {len(trim.sequence)}bp (trimmed {len(orig.sequence) - len(trim.sequence)}bp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae791b2",
   "metadata": {},
   "source": [
    "## 6. Quality-Based Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_by_quality_sliding_window(read: FASTQRecord, window_size: int = 5, min_quality: int = 20) -> FASTQRecord:\n",
    "    \"\"\"\n",
    "    Trim read using sliding window approach (similar to Trimmomatic).\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Scan read from 5' to 3' with a sliding window\n",
    "    2. If mean quality in window < threshold, trim from this position onwards\n",
    "    \"\"\"\n",
    "    phred_scores = read.get_phred_scores()\n",
    "    seq = read.sequence\n",
    "    qual = read.qualities\n",
    "    \n",
    "    # Scan from right to left (3' end)\n",
    "    for i in range(len(seq) - window_size, -1, -1):\n",
    "        window_quality = np.mean(phred_scores[i:i + window_size])\n",
    "        if window_quality < min_quality:\n",
    "            return FASTQRecord(read.header, seq[:i], read.plus, qual[:i])\n",
    "    \n",
    "    return read\n",
    "\n",
    "def trim_by_quality_per_base(read: FASTQRecord, min_quality: int = 20, leading: bool = True, trailing: bool = True) -> FASTQRecord:\n",
    "    \"\"\"\n",
    "    Trim low-quality bases from the read ends (leading and/or trailing).\n",
    "    \n",
    "    Args:\n",
    "        read: FASTQRecord to trim\n",
    "        min_quality: Minimum quality threshold\n",
    "        leading: Trim from 5' end\n",
    "        trailing: Trim from 3' end\n",
    "    \"\"\"\n",
    "    phred_scores = read.get_phred_scores()\n",
    "    seq = read.sequence\n",
    "    qual = read.qualities\n",
    "    \n",
    "    # Trim trailing low-quality bases\n",
    "    if trailing:\n",
    "        end = len(seq)\n",
    "        for i in range(len(seq) - 1, -1, -1):\n",
    "            if phred_scores[i] >= min_quality:\n",
    "                end = i + 1\n",
    "                break\n",
    "        seq = seq[:end]\n",
    "        qual = qual[:end]\n",
    "        phred_scores = phred_scores[:end]\n",
    "    \n",
    "    # Trim leading low-quality bases\n",
    "    if leading:\n",
    "        start = 0\n",
    "        for i in range(len(seq)):\n",
    "            if phred_scores[i] >= min_quality:\n",
    "                start = i\n",
    "                break\n",
    "        seq = seq[start:]\n",
    "        qual = qual[start:]\n",
    "    \n",
    "    return FASTQRecord(read.header, seq, read.plus, qual)\n",
    "\n",
    "def quality_filter(reads: List[FASTQRecord], min_quality: int = 20, method: str = 'sliding_window') -> Tuple[List[FASTQRecord], dict]:\n",
    "    \"\"\"\n",
    "    Apply quality trimming to all reads.\n",
    "    \n",
    "    Methods:\n",
    "    - 'per_base': Trim leading/trailing bases below threshold\n",
    "    - 'sliding_window': Use sliding window approach\n",
    "    \"\"\"\n",
    "    filtered_reads = []\n",
    "    stats = {'total': 0, 'kept': 0, 'removed': 0, 'trimmed_bases': 0, 'min_length': 100}\n",
    "    \n",
    "    for read in reads:\n",
    "        stats['total'] += 1\n",
    "        \n",
    "        # Apply trimming\n",
    "        if method == 'per_base':\n",
    "            trimmed = trim_by_quality_per_base(read, min_quality)\n",
    "        else:  # sliding_window\n",
    "            trimmed = trim_by_quality_sliding_window(read, min_quality=min_quality)\n",
    "        \n",
    "        # Check minimum length\n",
    "        if len(trimmed.sequence) >= stats['min_length']:\n",
    "            filtered_reads.append(trimmed)\n",
    "            stats['kept'] += 1\n",
    "            stats['trimmed_bases'] += len(read.sequence) - len(trimmed.sequence)\n",
    "        else:\n",
    "            stats['removed'] += 1\n",
    "    \n",
    "    return filtered_reads, stats\n",
    "\n",
    "# Apply quality filtering\n",
    "filtered_reads, filter_stats = quality_filter(synthetic_reads, min_quality=20, method='sliding_window')\n",
    "\n",
    "print(f\"Quality-Based Filtering Results:\")\n",
    "print(f\"  Total reads: {filter_stats['total']}\")\n",
    "print(f\"  Kept: {filter_stats['kept']} ({filter_stats['kept']/filter_stats['total']*100:.1f}%)\")\n",
    "print(f\"  Removed (too short): {filter_stats['removed']}\")\n",
    "print(f\"  Total bases trimmed: {filter_stats['trimmed_bases']:,}\")\n",
    "\n",
    "# Before/After stats\n",
    "before_stats = analyze_quality(synthetic_reads)\n",
    "after_stats = analyze_quality(filtered_reads)\n",
    "\n",
    "print(f\"\\nBefore and After:\")\n",
    "print(f\"  Read count: {before_stats['num_reads']:,} → {after_stats['num_reads']:,}\")\n",
    "print(f\"  Mean quality: {before_stats['mean_quality_mean']:.2f} → {after_stats['mean_quality_mean']:.2f}\")\n",
    "print(f\"  Min quality: {before_stats['min_quality_min']} → {after_stats['min_quality_min']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a1ccd",
   "metadata": {},
   "source": [
    "## 7. Complexity Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8781f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(sequence: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of a sequence.\n",
    "    High entropy = diverse bases, Low entropy = repetitive\n",
    "    \n",
    "    Formula: H = -Σ(p_i * log2(p_i)) where p_i is frequency of base i\n",
    "    Max entropy (DNA): 2 bits\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    \n",
    "    counts = Counter(sequence)\n",
    "    entropy = 0\n",
    "    for count in counts.values():\n",
    "        p = count / len(sequence)\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def homopolymer_runs(sequence: str) -> dict:\n",
    "    \"\"\"\n",
    "    Find homopolymer runs (stretches of identical bases).\n",
    "    Returns dict with max run length and count of runs > 5 bp.\n",
    "    \"\"\"\n",
    "    if len(sequence) == 0:\n",
    "        return {'max_run': 0, 'long_runs': 0}\n",
    "    \n",
    "    max_run = 1\n",
    "    current_run = 1\n",
    "    long_runs = 0\n",
    "    \n",
    "    for i in range(1, len(sequence)):\n",
    "        if sequence[i] == sequence[i-1]:\n",
    "            current_run += 1\n",
    "            max_run = max(max_run, current_run)\n",
    "        else:\n",
    "            if current_run > 5:\n",
    "                long_runs += 1\n",
    "            current_run = 1\n",
    "    \n",
    "    if current_run > 5:\n",
    "        long_runs += 1\n",
    "    \n",
    "    return {'max_run': max_run, 'long_runs': long_runs}\n",
    "\n",
    "def complexity_filter(reads: List[FASTQRecord], min_entropy: float = 1.5, max_homopolymer: int = 8) -> Tuple[List[FASTQRecord], dict]:\n",
    "    \"\"\"\n",
    "    Filter out low-complexity reads (repetitive or homopolymer-rich).\n",
    "    These reads are problematic for assembly.\n",
    "    \"\"\"\n",
    "    filtered_reads = []\n",
    "    stats = {'total': 0, 'kept': 0, 'removed_low_entropy': 0, 'removed_homopolymer': 0}\n",
    "    \n",
    "    for read in reads:\n",
    "        stats['total'] += 1\n",
    "        \n",
    "        entropy = shannon_entropy(read.sequence)\n",
    "        homo_info = homopolymer_runs(read.sequence)\n",
    "        \n",
    "        if entropy < min_entropy:\n",
    "            stats['removed_low_entropy'] += 1\n",
    "        elif homo_info['max_run'] > max_homopolymer:\n",
    "            stats['removed_homopolymer'] += 1\n",
    "        else:\n",
    "            filtered_reads.append(read)\n",
    "            stats['kept'] += 1\n",
    "    \n",
    "    return filtered_reads, stats\n",
    "\n",
    "# Apply complexity filter\n",
    "complex_filtered, complex_stats = complexity_filter(filtered_reads, min_entropy=1.5)\n",
    "\n",
    "print(f\"Complexity Filtering Results:\")\n",
    "print(f\"  Total reads: {complex_stats['total']}\")\n",
    "print(f\"  Kept: {complex_stats['kept']} ({complex_stats['kept']/complex_stats['total']*100:.1f}%)\")\n",
    "print(f\"  Removed (low entropy): {complex_stats['removed_low_entropy']}\")\n",
    "print(f\"  Removed (long homopolymers): {complex_stats['removed_homopolymer']}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nComplexity examples:\")\n",
    "for i in [0, 100, 200]:\n",
    "    if i < len(filtered_reads):\n",
    "read = filtered_reads[i]\n",
    "entropy = shannon_entropy(read.sequence)\n",
    "homo = homopolymer_runs(read.sequence)\n",
    "print(f\"  Read {i}: entropy={entropy:.2f}, max_homopolymer={homo['max_run']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3990450",
   "metadata": {},
   "source": [
    "## 8. Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(reads: List[FASTQRecord]) -> dict:\n",
    "    \"\"\"\n",
    "    Identify duplicate reads (exact sequence matches).\n",
    "    Returns dict with duplicate statistics and lists of duplicate clusters.\n",
    "    \"\"\"\n",
    "    sequence_map = defaultdict(list)\n",
    "    \n",
    "    # Map sequences to read indices\n",
    "    for idx, read in enumerate(reads):\n",
    "        sequence_map[read.sequence].append(idx)\n",
    "    \n",
    "    # Count duplicates\n",
    "    duplicate_clusters = [indices for indices in sequence_map.values() if len(indices) > 1]\n",
    "    num_duplicates = sum(len(indices) - 1 for indices in duplicate_clusters)\n",
    "    \n",
    "    return {\n",
    "        'total_reads': len(reads),\n",
    "        'unique_sequences': len(sequence_map),\n",
    "        'duplicate_reads': num_duplicates,\n",
    "        'duplicate_clusters': duplicate_clusters,\n",
    "        'sequence_map': sequence_map\n",
    "    }\n",
    "\n",
    "def deduplicate(reads: List[FASTQRecord], keep_first: bool = True) -> Tuple[List[FASTQRecord], dict]:\n",
    "    \"\"\"\n",
    "    Remove duplicate reads, optionally keeping the best quality copy.\n",
    "    \"\"\"\n",
    "    sequence_map = defaultdict(list)\n",
    "    \n",
    "    # Group by sequence\n",
    "    for idx, read in enumerate(reads):\n",
    "        sequence_map[read.sequence].append((idx, read))\n",
    "    \n",
    "    dedup_reads = []\n",
    "    stats = {'total': 0, 'unique': 0, 'duplicates_removed': 0}\n",
    "    \n",
    "    for sequence, read_list in sequence_map.items():\n",
    "        stats['total'] += len(read_list)\n",
    "        stats['unique'] += 1\n",
    "        \n",
    "        if len(read_list) > 1:\n",
    "            stats['duplicates_removed'] += len(read_list) - 1\n",
    "            # Keep read with highest mean quality\n",
    "            best_read = max(read_list, key=lambda x: x[1].mean_quality())[1]\n",
    "            dedup_reads.append(best_read)\n",
    "        else:\n",
    "            dedup_reads.append(read_list[0][1])\n",
    "    \n",
    "    return dedup_reads, stats\n",
    "\n",
    "# Test deduplication on a subset\n",
    "test_dedup = complex_filtered[:1000].copy()\n",
    "# Add some duplicates\n",
    "test_dedup.extend(test_dedup[10:15])\n",
    "\n",
    "dup_info = find_duplicates(test_dedup)\n",
    "print(f\"Duplicate Analysis (before dedup):\")\n",
    "print(f\"  Total reads: {dup_info['total_reads']}\")\n",
    "print(f\"  Unique sequences: {dup_info['unique_sequences']}\")\n",
    "print(f\"  Duplicate reads: {dup_info['duplicate_reads']}\")\n",
    "print(f\"  Largest duplicate cluster: {max((len(c) for c in dup_info['duplicate_clusters']), default=0)} copies\")\n",
    "\n",
    "dedup_reads, dedup_stats = deduplicate(test_dedup)\n",
    "print(f\"\\nAfter Deduplication:\")\n",
    "print(f\"  Total reads: {dedup_stats['total']}\")\n",
    "print(f\"  Unique reads: {dedup_stats['unique']}\")\n",
    "print(f\"  Duplicates removed: {dedup_stats['duplicates_removed']}\")\n",
    "print(f\"  Reads retained: {len(dedup_reads)} ({len(dedup_reads)/dedup_stats['total']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b1fc9",
   "metadata": {},
   "source": [
    "## 9. Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reads(input_reads: List[FASTQRecord], \n",
    "                    min_quality: int = 20,\n",
    "                    min_length: int = 50,\n",
    "                    remove_adapters: bool = True,\n",
    "                    remove_duplicates: bool = True,\n",
    "                    remove_low_complexity: bool = True) -> Tuple[List[FASTQRecord], dict]:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline:\n",
    "    1. Adapter trimming\n",
    "    2. Quality trimming\n",
    "    3. Length filtering\n",
    "    4. Complexity filtering\n",
    "    5. Deduplication\n",
    "    \"\"\"\n",
    "    pipeline_stats = {}\n",
    "    reads = input_reads\n",
    "    \n",
    "    print(f\"Starting preprocessing with {len(reads)} reads...\\n\")\n",
    "    \n",
    "    # Step 1: Adapter trimming\n",
    "    if remove_adapters:\n",
    "        reads, stats = trim_adapters(reads)\n",
    "        pipeline_stats['adapters'] = stats\n",
    "        print(f\"After adapter trimming: {stats['total'] - stats['trimmed']} reads unchanged, \"\n",
    "              f\"{stats['trimmed']} trimmed\")\n",
    "    \n",
    "    # Step 2: Quality trimming\n",
    "    reads, stats = quality_filter(reads, min_quality=min_quality, method='sliding_window')\n",
    "    pipeline_stats['quality'] = stats\n",
    "    print(f\"After quality trimming: {stats['kept']} reads retained (removed {stats['removed']} short reads)\")\n",
    "    \n",
    "    # Step 3: Complexity filtering\n",
    "    if remove_low_complexity:\n",
    "        reads, stats = complexity_filter(reads, min_entropy=1.5)\n",
    "        pipeline_stats['complexity'] = stats\n",
    "        print(f\"After complexity filtering: {stats['kept']} reads retained \"\n",
    "              f\"(removed {stats['removed_low_entropy'] + stats['removed_homopolymer']})\")\n",
    "    \n",
    "    # Step 4: Deduplication\n",
    "    if remove_duplicates:\n",
    "        reads, stats = deduplicate(reads)\n",
    "        pipeline_stats['duplicates'] = stats\n",
    "        print(f\"After deduplication: {stats['unique']} unique reads \"\n",
    "              f\"({stats['duplicates_removed']} duplicates removed)\")\n",
    "    \n",
    "    print(f\"\\nFinal: {len(reads)} reads for assembly\")\n",
    "    pipeline_stats['final_count'] = len(reads)\n",
    "    pipeline_stats['initial_count'] = len(input_reads)\n",
    "    \n",
    "    return reads, pipeline_stats\n",
    "\n",
    "# Run full pipeline\n",
    "print(\"=\"*50)\n",
    "print(\"FULL PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "preprocessed_reads, pp_stats = preprocess_reads(\n",
    "    synthetic_reads[:2000],  # Use subset for demo\n",
    "    min_quality=20,\n",
    "    min_length=50,\n",
    "    remove_adapters=False,  # No adapters in synthetic data\n",
    "    remove_duplicates=True,\n",
    "    remove_low_complexity=True\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"Summary: {pp_stats['initial_count']} → {pp_stats['final_count']} reads \"\n",
    "      f\"({pp_stats['final_count']/pp_stats['initial_count']*100:.1f}% retained)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff8532",
   "metadata": {},
   "source": [
    "## 10. Real Data Source Guide\n",
    "\n",
    "To use real genome sequencing data:\n",
    "\n",
    "**1. SRA (NCBI Sequence Read Archive)**\n",
    "   - Website: https://www.ncbi.nlm.nih.gov/sra\n",
    "   - Download using: `fastq-dump` or `fasterq-dump` (SRA Toolkit)\n",
    "   - Example: Bacterial genome with short reads\n",
    "\n",
    "**2. ENA (European Nucleotide Archive)**\n",
    "   - Website: https://www.ebi.ac.uk/ena\n",
    "   - Direct FASTQ downloads available\n",
    "\n",
    "**3. Public Databases**\n",
    "   - 1000 Genomes Project (human)\n",
    "   - Genome in a Bottle (high-confidence benchmarks)\n",
    "   - IMG/VR (viral genomes)\n",
    "\n",
    "**Example**: Download bacterial reads from SRA:\n",
    "```bash\n",
    "# Install SRA Toolkit first\n",
    "conda install -c bioconda sra-tools\n",
    "\n",
    "# Download specific run (e.g., SRR1111111)\n",
    "fasterq-dump SRR1111111 -O ./reads --threads 8\n",
    "```\n",
    "\n",
    "The preprocessing code above will work with real FASTQ files without modification!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567ebee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. **Adapter trimming**: Remove ligated DNA sequences\n",
    "2. **Quality trimming**: Remove low-quality bases using sliding window or per-base filtering\n",
    "3. **Length filtering**: Discard reads that become too short\n",
    "4. **Complexity filtering**: Remove repetitive/low-entropy reads\n",
    "5. **Deduplication**: Remove exact duplicates\n",
    "\n",
    "**Key metrics:**\n",
    "- Quality score distribution by position\n",
    "- GC content vs. expected (indicates contamination)\n",
    "- Read length distribution\n",
    "\n",
    "**Next**: Notebook 3 covers the actual assembly algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
